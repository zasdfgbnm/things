{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0a0+dedd16b\n",
      "dedd16b4181cae81e37e978cd3bf24c1ba35ca05\n",
      "\n",
      "Running:\n",
      "rm -rf BinaryArithmeticKernel.sm_70.cubin; cuobjdump -xelf BinaryArithmeticKernel.sm_70.cubin /home/xgao/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so\n",
      "\n",
      "**Symbol:**\n",
      "void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&)::{lambda(int)#2}>(int, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&)::{lambda(int)#2})\n",
      "\n",
      "**ASM:**\n",
      "\n",
      "\t.section\t.text._ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS4_RKT_EUliE0_EEviT1_,\"ax\",@progbits\n",
      "\t.sectioninfo\t@\"SHI_REGISTERS=12\"\n",
      "\t.align\t128\n",
      "        .global         _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS4_RKT_EUliE0_EEviT1_\n",
      "        .type           _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS4_RKT_EUliE0_EEviT1_,@function\n",
      "        .size           _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS4_RKT_EUliE0_EEviT1_,(.L_29063 - _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS4_RKT_EUliE0_EEviT1_)\n",
      "        .other          _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS4_RKT_EUliE0_EEviT1_,@\"STO_CUDA_ENTRY STV_DEFAULT\"\n",
      "_ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS4_RKT_EUliE0_EEviT1_:\n",
      ".text._ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS4_RKT_EUliE0_EEviT1_:\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 71\n",
      "        /*0000*/                   IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;\n",
      "        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;\n",
      "        /*0020*/                   S2R R0, SR_TID.X ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 73\n",
      "        /*0030*/                   S2R R3, SR_CTAID.X ;\n",
      "        /*0040*/                   IMAD R0, R3, 0x200, R0 ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 76\n",
      "        /*0050*/                   ISETP.GE.AND P0, PT, R0, c[0x0][0x160], PT ;\n",
      "        /*0060*/               @P0 EXIT ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 110\n",
      "        /*0070*/                   IMAD R3, R0.reuse, c[0x0][0x194], RZ ;\n",
      "        /*0080*/                   IMAD R6, R0, c[0x0][0x198], RZ ;\n",
      "        /*0090*/                   IADD3 R4, P0, R3.reuse, c[0x0][0x178], RZ ;\n",
      "        /*00a0*/                   IADD3 R2, P1, R6.reuse, c[0x0][0x180], RZ ;\n",
      "        /*00b0*/                   LEA.HI.X.SX32 R5, R3, c[0x0][0x17c], 0x1, P0 ;\n",
      "        /*00c0*/                   LEA.HI.X.SX32 R3, R6, c[0x0][0x184], 0x1, P1 ;\n",
      "        /*00d0*/                   LDG.E.SYS R5, [R4] ;\n",
      "        /*00e0*/                   LDG.E.SYS R2, [R2] ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 77\n",
      "        /*00f0*/                   IMAD R0, R0, c[0x0][0x190], RZ ;\n",
      "        /*0100*/                   IADD3 R6, P0, R0, c[0x0][0x170], RZ ;\n",
      "        /*0110*/                   LEA.HI.X.SX32 R7, R0, c[0x0][0x174], 0x1, P0 ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 110\n",
      "        /*0120*/                   FFMA R9, R2, c[0x0][0x1a0], R5 ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 170\n",
      "        /*0130*/                   STG.E.SYS [R6], R9 ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 81\n",
      "        /*0140*/                   EXIT ;\n",
      ".L_16826:\n",
      "        /*0150*/                   BRA `(.L_16826);\n",
      "        /*0160*/                   NOP;\n",
      "        /*0170*/                   NOP;\n",
      ".L_29063:\n",
      "\n",
      "**Symbol:**\n",
      "void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&)::{lambda(float)#2}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1})::{lambda(int)#2}>(int, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&)::{lambda(float)#2}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1})::{lambda(int)#2})\n",
      "\n",
      "**ASM:**\n",
      "\n",
      "\t.section\t.text._ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_EEvS5_SD_EUliE0_EEviT1_,\"ax\",@progbits\n",
      "\t.sectioninfo\t@\"SHI_REGISTERS=10\"\n",
      "\t.align\t128\n",
      "        .global         _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_EEvS5_SD_EUliE0_EEviT1_\n",
      "        .type           _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_EEvS5_SD_EUliE0_EEviT1_,@function\n",
      "        .size           _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_EEvS5_SD_EUliE0_EEviT1_,(.L_29067 - _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_EEvS5_SD_EUliE0_EEviT1_)\n",
      "        .other          _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_EEvS5_SD_EUliE0_EEviT1_,@\"STO_CUDA_ENTRY STV_DEFAULT\"\n",
      "_ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_EEvS5_SD_EUliE0_EEviT1_:\n",
      ".text._ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_EEvS5_SD_EUliE0_EEviT1_:\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 73\n",
      "        /*0000*/                   IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;\n",
      "        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;\n",
      "        /*0020*/                   S2R R0, SR_CTAID.X ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 71\n",
      "        /*0030*/                   S2R R3, SR_TID.X ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 73\n",
      "        /*0040*/                   LEA R0, R0, R3, 0x9 ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 76\n",
      "        /*0050*/                   ISETP.GE.AND P0, PT, R0, c[0x0][0x160], PT ;\n",
      "        /*0060*/               @P0 EXIT ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 110\n",
      "        /*0070*/                   IMAD R3, R0, c[0x0][0x184], RZ ;\n",
      "        /*0080*/                   IADD3 R2, P0, R3, c[0x0][0x178], RZ ;\n",
      "        /*0090*/                   LEA.HI.X.SX32 R3, R3, c[0x0][0x17c], 0x1, P0 ;\n",
      "        /*00a0*/                   LDG.E.SYS R2, [R2] ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 77\n",
      "        /*00b0*/                   IMAD R0, R0, c[0x0][0x180], RZ ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 110\n",
      "        /*00c0*/                   IMAD.MOV.U32 R7, RZ, RZ, c[0x0][0x190] ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 77\n",
      "        /*00d0*/                   IADD3 R4, P0, R0, c[0x0][0x170], RZ ;\n",
      "        /*00e0*/                   LEA.HI.X.SX32 R5, R0, c[0x0][0x174], 0x1, P0 ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 110\n",
      "        /*00f0*/                   FFMA R7, R7, c[0x0][0x1a0], R2 ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 170\n",
      "        /*0100*/                   STG.E.SYS [R4], R7 ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 81\n",
      "        /*0110*/                   EXIT ;\n",
      ".L_16980:\n",
      "        /*0120*/                   BRA `(.L_16980);\n",
      "        /*0130*/                   NOP;\n",
      "        /*0140*/                   NOP;\n",
      "        /*0150*/                   NOP;\n",
      "        /*0160*/                   NOP;\n",
      "        /*0170*/                   NOP;\n",
      ".L_29067:\n",
      "\n",
      "**Symbol:**\n",
      "void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&)::{lambda(float)#1}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1})::{lambda(int)#2}>(int, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&)::{lambda(float)#1}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1})::{lambda(int)#2})\n",
      "\n",
      "**ASM:**\n",
      "\n",
      "\t.section\t.text._ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_EEvS5_SD_EUliE0_EEviT1_,\"ax\",@progbits\n",
      "\t.sectioninfo\t@\"SHI_REGISTERS=10\"\n",
      "\t.align\t128\n",
      "        .global         _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_EEvS5_SD_EUliE0_EEviT1_\n",
      "        .type           _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_EEvS5_SD_EUliE0_EEviT1_,@function\n",
      "        .size           _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_EEvS5_SD_EUliE0_EEviT1_,(.L_29071 - _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_EEvS5_SD_EUliE0_EEviT1_)\n",
      "        .other          _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_EEvS5_SD_EUliE0_EEviT1_,@\"STO_CUDA_ENTRY STV_DEFAULT\"\n",
      "_ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_EEvS5_SD_EUliE0_EEviT1_:\n",
      ".text._ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_EEvS5_SD_EUliE0_EEviT1_:\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 71\n",
      "        /*0000*/                   IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;\n",
      "        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;\n",
      "        /*0020*/                   S2R R0, SR_TID.X ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 73\n",
      "        /*0030*/                   S2R R3, SR_CTAID.X ;\n",
      "        /*0040*/                   LEA R0, R3, R0, 0x9 ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 76\n",
      "        /*0050*/                   ISETP.GE.AND P0, PT, R0, c[0x0][0x160], PT ;\n",
      "        /*0060*/               @P0 EXIT ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 110\n",
      "        /*0070*/                   IMAD R3, R0, c[0x0][0x184], RZ ;\n",
      "        /*0080*/                   IADD3 R2, P0, R3, c[0x0][0x178], RZ ;\n",
      "        /*0090*/                   LEA.HI.X.SX32 R3, R3, c[0x0][0x17c], 0x1, P0 ;\n",
      "        /*00a0*/                   LDG.E.SYS R2, [R2] ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 77\n",
      "        /*00b0*/                   IMAD R0, R0, c[0x0][0x180], RZ ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 110\n",
      "        /*00c0*/                   IMAD.MOV.U32 R7, RZ, RZ, c[0x0][0x190] ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 77\n",
      "        /*00d0*/                   IADD3 R4, P0, R0, c[0x0][0x170], RZ ;\n",
      "        /*00e0*/                   LEA.HI.X.SX32 R5, R0, c[0x0][0x174], 0x1, P0 ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 110\n",
      "        /*00f0*/                   FFMA R7, R2, R7, c[0x0][0x1a0] ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 170\n",
      "        /*0100*/                   STG.E.SYS [R4], R7 ;\n",
      "\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 81\n",
      "        /*0110*/                   EXIT ;\n",
      ".L_17122:\n",
      "        /*0120*/                   BRA `(.L_17122);\n",
      "        /*0130*/                   NOP;\n",
      "        /*0140*/                   NOP;\n",
      "        /*0150*/                   NOP;\n",
      "        /*0160*/                   NOP;\n",
      "        /*0170*/                   NOP;\n",
      ".L_29071:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import disasm\n",
    "\n",
    "def filter_(demangled_symbol, code):\n",
    "    ret = True\n",
    "    ret &= 'add_kernel' in demangled_symbol             # add kernel\n",
    "    ret &= 'lambda(float, float)' in demangled_symbol   # dtype = float\n",
    "    ret &= 'TypeCast.h' not in code                     # no dynamic casting\n",
    "    ret &= 'OffsetCalculator.cuh' not in code           # trivial 1d\n",
    "    return ret\n",
    "\n",
    "disasm.run('BinaryArithmeticKernel.sm_70.cubin', filter_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0a0+b1a239b\n",
      "b1a239be8d529e89875fe47cd09964ef3a9516ac\n",
      "\n",
      "Running:\n",
      "rm -rf BinaryArithmeticKernel.sm_70.cubin; cuobjdump -xelf BinaryArithmeticKernel.sm_70.cubin /home/xgao/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so\n",
      "\n",
      "**Symbol:**\n",
      "void at::native::modern::elementwise_kernel<64, 4, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>, 0>(int, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)\n",
      "\n",
      "**ASM:**\n",
      "\n",
      "\t.section\t.text._ZN2at6native6modern18elementwise_kernelILi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEELi0EEEviT1_T2_,\"ax\",@progbits\n",
      "\t.sectioninfo\t@\"SHI_REGISTERS=18\"\n",
      "\t.align\t128\n",
      "        .global         _ZN2at6native6modern18elementwise_kernelILi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEELi0EEEviT1_T2_\n",
      "        .type           _ZN2at6native6modern18elementwise_kernelILi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEELi0EEEviT1_T2_,@function\n",
      "        .size           _ZN2at6native6modern18elementwise_kernelILi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEELi0EEEviT1_T2_,(.L_32233 - _ZN2at6native6modern18elementwise_kernelILi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEELi0EEEviT1_T2_)\n",
      "        .other          _ZN2at6native6modern18elementwise_kernelILi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEELi0EEEviT1_T2_,@\"STO_CUDA_ENTRY STV_DEFAULT\"\n",
      "_ZN2at6native6modern18elementwise_kernelILi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEELi0EEEviT1_T2_:\n",
      ".text._ZN2at6native6modern18elementwise_kernelILi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEELi0EEEviT1_T2_:\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 210\n",
      "        /*0000*/                   MOV R1, c[0x0][0x28] ;\n",
      "        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;\n",
      "        /*0020*/                   S2R R6, SR_CTAID.X ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 217\n",
      "        /*0030*/                   MOV R7, 0x4 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 208\n",
      "        /*0040*/                   S2R R3, SR_TID.X ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 210\n",
      "        /*0050*/                   LEA R6, R6, R3, 0x8 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 225\n",
      "        /*0060*/                   IADD3 R2, R6.reuse, 0x40, RZ ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 217\n",
      "        /*0070*/                   IMAD.WIDE R4, R6.reuse, R7.reuse, c[0x0][0x190] ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 225\n",
      "        /*0080*/                   IADD3 R3, R6, 0x80, RZ ;\n",
      "        /*0090*/                   ISETP.GE.AND P1, PT, R2, c[0x0][0x160], PT ;\n",
      "        /*00a0*/                   ISETP.GE.AND P0, PT, R6.reuse, c[0x0][0x160], PT ;\n",
      "        /*00b0*/                   ISETP.GE.AND P2, PT, R3, c[0x0][0x160], PT ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 217\n",
      "        /*00c0*/                   IMAD.WIDE R2, R6.reuse, R7, c[0x0][0x188] ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 225\n",
      "        /*00d0*/                   IADD3 R14, R6, 0xc0, RZ ;\n",
      "        /*00e0*/                   ISETP.GE.AND P3, PT, R14, c[0x0][0x160], PT ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 228\n",
      "        /*00f0*/              @!P1 LDG.E.SYS R11, [R4+0x100] ;\n",
      "        /*0100*/              @!P0 LDG.E.SYS R0, [R2] ;\n",
      "        /*0110*/              @!P0 LDG.E.SYS R9, [R4] ;\n",
      "        /*0120*/              @!P1 LDG.E.SYS R8, [R2+0x100] ;\n",
      "        /*0130*/              @!P2 LDG.E.SYS R10, [R2+0x200] ;\n",
      "        /*0140*/              @!P2 LDG.E.SYS R13, [R4+0x200] ;\n",
      "        /*0150*/              @!P3 LDG.E.SYS R12, [R2+0x300] ;\n",
      "        /*0160*/              @!P3 LDG.E.SYS R15, [R4+0x300] ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n",
      "        /*0170*/                   IMAD.WIDE R6, R6, R7, c[0x0][0x180] ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 191\n",
      "        /*0180*/                   FFMA R9, R9, c[0x0][0x168], R0 ;\n",
      "        /*0190*/                   FFMA R11, R11, c[0x0][0x168], R8 ;\n",
      "        /*01a0*/                   FFMA R13, R13, c[0x0][0x168], R10 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n",
      "        /*01b0*/              @!P0 STG.E.SYS [R6], R9 ;\n",
      "        /*01c0*/              @!P1 STG.E.SYS [R6+0x100], R11 ;\n",
      "        /*01d0*/              @!P2 STG.E.SYS [R6+0x200], R13 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 191\n",
      "        /*01e0*/                   FFMA R15, R15, c[0x0][0x168], R12 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 244\n",
      "        /*01f0*/               @P3 EXIT ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n",
      "        /*0200*/                   STG.E.SYS [R6+0x300], R15 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 248\n",
      "        /*0210*/                   EXIT ;\n",
      ".L_727:\n",
      "        /*0220*/                   BRA `(.L_727);\n",
      "        /*0230*/                   NOP;\n",
      "        /*0240*/                   NOP;\n",
      "        /*0250*/                   NOP;\n",
      "        /*0260*/                   NOP;\n",
      "        /*0270*/                   NOP;\n",
      ".L_32233:\n",
      "\n",
      "**Symbol:**\n",
      "void at::native::modern::elementwise_kernel<64, 4, at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&)::{lambda(float)#2}, at::detail::Array<char*, 2>, 0>(int, at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&)::{lambda(float)#2}, at::detail::Array<char*, 2>)\n",
      "\n",
      "**ASM:**\n",
      "\n",
      "\t.section\t.text._ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_,\"ax\",@progbits\n",
      "\t.sectioninfo\t@\"SHI_REGISTERS=16\"\n",
      "\t.align\t128\n",
      "        .global         _ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_\n",
      "        .type           _ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_,@function\n",
      "        .size           _ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_,(.L_32234 - _ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_)\n",
      "        .other          _ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_,@\"STO_CUDA_ENTRY STV_DEFAULT\"\n",
      "_ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_:\n",
      ".text._ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE0_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_:\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 210\n",
      "        /*0000*/                   IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;\n",
      "        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;\n",
      "        /*0020*/                   S2R R0, SR_CTAID.X ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 208\n",
      "        /*0030*/                   S2R R3, SR_TID.X ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 210\n",
      "        /*0040*/                   IMAD R0, R0, 0x100, R3 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 217\n",
      "        /*0050*/                   IMAD.MOV.U32 R3, RZ, RZ, 0x4 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 225\n",
      "        /*0060*/                   IADD3 R2, R0.reuse, 0x40, RZ ;\n",
      "        /*0070*/                   IADD3 R4, R0, 0x80, RZ ;\n",
      "        /*0080*/                   ISETP.GE.AND P1, PT, R2, c[0x0][0x160], PT ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 217\n",
      "        /*0090*/                   IMAD.WIDE R2, R0, R3, c[0x0][0x198] ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 225\n",
      "        /*00a0*/                   ISETP.GE.AND P2, PT, R4, c[0x0][0x160], PT ;\n",
      "        /*00b0*/                   ISETP.GE.AND P0, PT, R0.reuse, c[0x0][0x160], PT ;\n",
      "        /*00c0*/                   IADD3 R4, R0, 0xc0, RZ ;\n",
      "        /*00d0*/                   ISETP.GE.AND P3, PT, R4, c[0x0][0x160], PT ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 228\n",
      "        /*00e0*/              @!P1 LDG.E.SYS R7, [R2+0x100] ;\n",
      "        /*00f0*/              @!P0 LDG.E.SYS R6, [R2] ;\n",
      "        /*0100*/              @!P2 LDG.E.SYS R8, [R2+0x200] ;\n",
      "        /*0110*/              @!P3 LDG.E.SYS R9, [R2+0x300] ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 191\n",
      "        /*0120*/                   IMAD.MOV.U32 R10, RZ, RZ, c[0x0][0x168] ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 213\n",
      "        /*0130*/                   SHF.R.S32.HI R5, RZ, 0x1f, R0 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n",
      "        /*0140*/                   LEA R4, P4, R0, c[0x0][0x190], 0x2 ;\n",
      "        /*0150*/                   LEA.HI.X R5, R0, c[0x0][0x194], R5, 0x2, P4 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 191\n",
      "        /*0160*/                   FFMA R7, R10.reuse, c[0x0][0x178], R7 ;\n",
      "        /*0170*/                   FFMA R11, R10.reuse, c[0x0][0x178], R6 ;\n",
      "        /*0180*/                   FFMA R13, R10, c[0x0][0x178], R8 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n",
      "        /*0190*/              @!P1 STG.E.SYS [R4+0x100], R7 ;\n",
      "        /*01a0*/              @!P0 STG.E.SYS [R4], R11 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 191\n",
      "        /*01b0*/                   FFMA R9, R10, c[0x0][0x178], R9 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n",
      "        /*01c0*/              @!P2 STG.E.SYS [R4+0x200], R13 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 244\n",
      "        /*01d0*/               @P3 EXIT ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n",
      "        /*01e0*/                   STG.E.SYS [R4+0x300], R9 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 248\n",
      "        /*01f0*/                   EXIT ;\n",
      ".L_728:\n",
      "        /*0200*/                   BRA `(.L_728);\n",
      "        /*0210*/                   NOP;\n",
      "        /*0220*/                   NOP;\n",
      "        /*0230*/                   NOP;\n",
      "        /*0240*/                   NOP;\n",
      "        /*0250*/                   NOP;\n",
      "        /*0260*/                   NOP;\n",
      "        /*0270*/                   NOP;\n",
      ".L_32234:\n",
      "\n",
      "**Symbol:**\n",
      "void at::native::modern::elementwise_kernel<64, 4, at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&)::{lambda(float)#1}, at::detail::Array<char*, 2>, 0>(int, at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}>(at::TensorIterator&, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&)::{lambda(float)#1}, at::detail::Array<char*, 2>)\n",
      "\n",
      "**ASM:**\n",
      "\n",
      "\t.section\t.text._ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_,\"ax\",@progbits\n",
      "\t.sectioninfo\t@\"SHI_REGISTERS=16\"\n",
      "\t.align\t128\n",
      "        .global         _ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_\n",
      "        .type           _ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_,@function\n",
      "        .size           _ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_,(.L_32235 - _ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_)\n",
      "        .other          _ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_,@\"STO_CUDA_ENTRY STV_DEFAULT\"\n",
      "_ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_:\n",
      ".text._ZN2at6native6modern18elementwise_kernelILi64ELi4EZNS0_23gpu_kernel_with_scalarsIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS5_RKT_EUlfE_NS_6detail5ArrayIPcLi2EEELi0EEEviT1_T2_:\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 210\n",
      "        /*0000*/                   IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;\n",
      "        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;\n",
      "        /*0020*/                   S2R R0, SR_CTAID.X ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 208\n",
      "        /*0030*/                   S2R R3, SR_TID.X ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 210\n",
      "        /*0040*/                   IMAD R0, R0, 0x100, R3 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 217\n",
      "        /*0050*/                   IMAD.MOV.U32 R3, RZ, RZ, 0x4 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 225\n",
      "        /*0060*/                   IADD3 R2, R0.reuse, 0x40, RZ ;\n",
      "        /*0070*/                   IADD3 R4, R0, 0x80, RZ ;\n",
      "        /*0080*/                   ISETP.GE.AND P1, PT, R2, c[0x0][0x160], PT ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 217\n",
      "        /*0090*/                   IMAD.WIDE R2, R0, R3, c[0x0][0x198] ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 225\n",
      "        /*00a0*/                   ISETP.GE.AND P2, PT, R4, c[0x0][0x160], PT ;\n",
      "        /*00b0*/                   ISETP.GE.AND P0, PT, R0.reuse, c[0x0][0x160], PT ;\n",
      "        /*00c0*/                   IADD3 R4, R0, 0xc0, RZ ;\n",
      "        /*00d0*/                   ISETP.GE.AND P3, PT, R4, c[0x0][0x160], PT ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 228\n",
      "        /*00e0*/              @!P1 LDG.E.SYS R7, [R2+0x100] ;\n",
      "        /*00f0*/              @!P0 LDG.E.SYS R6, [R2] ;\n",
      "        /*0100*/              @!P2 LDG.E.SYS R8, [R2+0x200] ;\n",
      "        /*0110*/              @!P3 LDG.E.SYS R9, [R2+0x300] ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 191\n",
      "        /*0120*/                   IMAD.MOV.U32 R10, RZ, RZ, c[0x0][0x168] ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 213\n",
      "        /*0130*/                   SHF.R.S32.HI R5, RZ, 0x1f, R0 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n",
      "        /*0140*/                   LEA R4, P4, R0, c[0x0][0x190], 0x2 ;\n",
      "        /*0150*/                   LEA.HI.X R5, R0, c[0x0][0x194], R5, 0x2, P4 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 191\n",
      "        /*0160*/                   FFMA R7, R7, R10.reuse, c[0x0][0x178] ;\n",
      "        /*0170*/                   FFMA R11, R6, R10.reuse, c[0x0][0x178] ;\n",
      "        /*0180*/                   FFMA R13, R8, R10, c[0x0][0x178] ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n",
      "        /*0190*/              @!P1 STG.E.SYS [R4+0x100], R7 ;\n",
      "        /*01a0*/              @!P0 STG.E.SYS [R4], R11 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 191\n",
      "        /*01b0*/                   FFMA R9, R9, R10, c[0x0][0x178] ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n",
      "        /*01c0*/              @!P2 STG.E.SYS [R4+0x200], R13 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 244\n",
      "        /*01d0*/               @P3 EXIT ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n",
      "        /*01e0*/                   STG.E.SYS [R4+0x300], R9 ;\n",
      "\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 248\n",
      "        /*01f0*/                   EXIT ;\n",
      ".L_729:\n",
      "        /*0200*/                   BRA `(.L_729);\n",
      "        /*0210*/                   NOP;\n",
      "        /*0220*/                   NOP;\n",
      "        /*0230*/                   NOP;\n",
      "        /*0240*/                   NOP;\n",
      "        /*0250*/                   NOP;\n",
      "        /*0260*/                   NOP;\n",
      "        /*0270*/                   NOP;\n",
      ".L_32235:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import disasm\n",
    "\n",
    "def filter_(demangled_symbol, code):\n",
    "    ret = True\n",
    "    ret &= 'modern' in demangled_symbol                 # new code\n",
    "    ret &= 'add_kernel' in demangled_symbol             # add kernel\n",
    "    ret &= 'lambda(float, float)' in demangled_symbol   # dtype = float\n",
    "    ret &= 'TypeCast.h' not in code                     # no dynamic casting\n",
    "    ret &= 'OffsetCalculator.cuh' not in code           # trivial 1d\n",
    "    return ret\n",
    "\n",
    "disasm.run('BinaryArithmeticKernel.sm_70.cubin', filter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
